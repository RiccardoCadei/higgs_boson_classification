{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GHhNTfWx0AGu"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from exploration import *\n",
    "from process_data import *\n",
    "from crossvalidation import *\n",
    "from select_parameter import *\n",
    "\n",
    "seed=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJdJ20A-0AG1"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dIS5My0U0AG1"
   },
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data('data/train.csv')\n",
    "_, tX_test, ids_test = load_csv_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGxjbAte0AG6"
   },
   "source": [
    "# Models\n",
    "\n",
    "We now apply the 6 methods expected for this project. We compare the performance of each method by performing cross-validation on the training set, to have an estimate of the test accuracy. Valuating the variance of the test accuracy predicted and comparing test accuracy predicted with training accuracy we can evaluate if our model is overfitting or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhDrrEpc0AG7"
   },
   "source": [
    "## 1. Least Squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6yMTilRb0AG8",
    "outputId": "2e810048-012d-4944-da60-96ffdeee0a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Training accuracy: 0.787600 / Test accuracy : 0.787160\n",
      "Iter 1: Training accuracy: 0.787330 / Test accuracy : 0.788160\n",
      "Iter 2: Training accuracy: 0.787775 / Test accuracy : 0.784340\n",
      "Iter 3: Training accuracy: 0.787615 / Test accuracy : 0.786120\n",
      "Iter 4: Training accuracy: 0.787000 / Test accuracy : 0.788820\n",
      "\n",
      "Average test accuracy: 0.786920\n",
      "Variance test accuracy: 0.000003\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [5, 5, 5]\n",
    "alphas = [4, 4, 5]\n",
    "\n",
    "# Model parameters\n",
    "max_iters = 500\n",
    "gamma = 0.00005\n",
    "\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, tX, least_squares_GD, k_indices, k, degrees, alphas,\n",
    "                                           max_iters=max_iters, gamma=gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkDIXO5f0AHD"
   },
   "source": [
    "## 2. Least Squares with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qMfrUMhn0AHF",
    "outputId": "9472c33e-9f7b-415b-b910-faced071a5ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Training accuracy: 0.715089 / Test accuracy : 0.716943\n",
      "Iter 1: Training accuracy: 0.721263 / Test accuracy : 0.719691\n",
      "Iter 2: Training accuracy: 0.715623 / Test accuracy : 0.716271\n",
      "\n",
      "Average test accuracy: 0.717635\n",
      "Variance test accuracy: 0.000002\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [5, 5, 5]\n",
    "alphas = [4, 4, 5]\n",
    "\n",
    "# Model parameters\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "batch_size=1\n",
    "\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 3\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, tX, least_squares_SGD, k_indices, k, degrees, alphas, \n",
    "                                           max_iters=max_iters, gamma=gamma, batch_size=batch_size)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trL4zoRQ0AHJ"
   },
   "source": [
    "## 3. Least Squares with Normal Equations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZBfwZaXh0AHL",
    "outputId": "394c5bce-a0d5-4128-90d9-16295bf00a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Training accuracy: 0.843625 / Test accuracy : 0.836580\n",
      "Iter 1: Training accuracy: 0.842920 / Test accuracy : 0.840440\n",
      "Iter 2: Training accuracy: 0.843120 / Test accuracy : 0.824320\n",
      "Iter 3: Training accuracy: 0.843460 / Test accuracy : 0.840900\n",
      "Iter 4: Training accuracy: 0.837225 / Test accuracy : 0.831080\n",
      "\n",
      "Average test accuracy: 0.834664\n",
      "Variance test accuracy: 0.000039\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [5, 5, 5]\n",
    "alphas = [4, 4, 5]\n",
    "\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, tX, least_squares, k_indices, k, degrees, alphas)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRNkS9Vt0AHP"
   },
   "source": [
    "## 4. Ridge regression with Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FSx3D5s0AHQ"
   },
   "source": [
    "### Grid Search to find the best parameters (Alpha, Lambda, Degree)  per class_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbx_ypfVIdS6",
    "outputId": "86f98907-ff32-4bda-a019-bd7be088aff5"
   },
   "outputs": [],
   "source": [
    "# canditates parameters\n",
    "degrees_candidates = [4,5,6]\n",
    "alphas_candidates=[3,4,5]\n",
    "lambdas_candidates = np.logspace(-3,-6,4)\n",
    "\n",
    "\n",
    "k_fold = 3\n",
    "\n",
    "opt_degree, opt_lambda, opt_alpha, accu = select_parameters_ridge_regression_jet(y,tX,degrees_candidates,lambdas_candidates,\n",
    "                                                                  alphas_candidates,k_fold,seed)\n",
    "print('Optimal alphas per jet_class:',opt_alpha)\n",
    "print('Optimal degrees per jet_class:',opt_degree)\n",
    "print('Optimal lambdas per jet_class:',opt_lambda)\n",
    "print('Maximum accuracy predicted per jet_class:',accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2K8LmbWb0AHX",
    "outputId": "33fa584e-49d2-48eb-89fd-6ac9da428a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Training accuracy: 0.843205 / Test accuracy : 0.838920\n",
      "Iter 1: Training accuracy: 0.842600 / Test accuracy : 0.841640\n",
      "Iter 2: Training accuracy: 0.843100 / Test accuracy : 0.839000\n",
      "Iter 3: Training accuracy: 0.843030 / Test accuracy : 0.840740\n",
      "Iter 4: Training accuracy: 0.842665 / Test accuracy : 0.842580\n",
      "\n",
      "Average test accuracy: 0.840576\n",
      "Variance test accuracy: 0.000002\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [5, 5, 5]\n",
    "alphas = [4, 4, 5]\n",
    "lambdas = [1e-06, 1e-05, 1e-03]\n",
    "\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, tX, ridge_regression, k_indices, k, degrees, alphas, lambdas)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0qlFWFL0AHb"
   },
   "source": [
    "## 5. Logistic Regression with Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tqdp4-eb0AHc",
    "outputId": "5eba52c4-27c6-461c-9865-61aadde2bd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Training accuracy: 0.710235 / Test accuracy : 0.710960\n",
      "Iter 1: Training accuracy: 0.722310 / Test accuracy : 0.720140\n",
      "Iter 2: Training accuracy: 0.724430 / Test accuracy : 0.720800\n",
      "Iter 3: Training accuracy: 0.722135 / Test accuracy : 0.722940\n",
      "Iter 4: Training accuracy: 0.681275 / Test accuracy : 0.680720\n",
      "\n",
      "Average test accuracy: 0.711112\n",
      "Variance test accuracy: 0.000248\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [5, 5, 5]\n",
    "alphas = [4, 4, 5]\n",
    "\n",
    "# Model parameters\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "batch_size = 1\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, tX, logistic_regression, k_indices, k, degrees, alphas, log=True,\n",
    "                                           batch_size=batch_size, max_iters=max_iters, gamma=gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGOLWOaT0AHg"
   },
   "source": [
    "## 6. Regularized Logistic Regression with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iy2RbgfS0AHh",
    "outputId": "301bdd41-0924-423a-f941-b7e368095593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Training accuracy: 0.710245 / Test accuracy : 0.710960\n",
      "Iter 1: Training accuracy: 0.722310 / Test accuracy : 0.720140\n",
      "Iter 2: Training accuracy: 0.724430 / Test accuracy : 0.720800\n",
      "Iter 3: Training accuracy: 0.722130 / Test accuracy : 0.722940\n",
      "Iter 4: Training accuracy: 0.681280 / Test accuracy : 0.680700\n",
      "\n",
      "Average test accuracy: 0.711108\n",
      "Variance test accuracy: 0.000248\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing parameters\n",
    "degrees = [5, 5, 5]\n",
    "alphas = [4, 4, 5]\n",
    "\n",
    "# Model parameters\n",
    "lambdas=[0.1,0.1,0.1]\n",
    "max_iters = 100\n",
    "gamma = 0.00001\n",
    "batch_size = 1\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_jet(y, tX, reg_logistic_regression, k_indices, k, degrees, alphas, lambdas, log=True,\n",
    "                                           batch_size=batch_size, max_iters=max_iters, gamma=gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"Iter %d: Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub5ITgmg0AH5"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0SDbDlvP7dC0"
   },
   "outputs": [],
   "source": [
    "! python run.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
