{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "from methods import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data('Data/train.csv')\n",
    "_, tX_test, ids_test = load_csv_data('Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 250000 training examples, 85667 are 1, i.e. the 0.342668 %\n"
     ]
    }
   ],
   "source": [
    "higgs = np.count_nonzero(y==1)\n",
    "print(f'From {y.shape[0]} training examples, {higgs} are 1, i.e. the {higgs/y.shape[0]} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: We could use Undersampling\\Oversampling alghoritms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, tX_test = missing_values(tX, tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPERIDEA: SLIT THE DATASET IN 4 CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, mean_tX, std_tX = standardize(tX)\n",
    "tX_test, _, _ = standardize(tX, mean_tX, std_tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ideas\n",
    "\n",
    "1. New features: Apply a polynomial basis to all the X features\n",
    "\n",
    "2. PCA, correlation analysis (scatterplot, VIF, ...), manage the 0s in the last feature\n",
    "\n",
    "3. Outlayer analysis, leverages, cook's metric ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, tX_test = process_data(tX, tX_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Least Squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/1000): loss=11.408273212558187\n",
      "Gradient Descent(100/1000): loss=1.038268212014649\n",
      "Gradient Descent(200/1000): loss=0.5490861839099093\n",
      "Gradient Descent(300/1000): loss=0.4253527070169913\n",
      "Gradient Descent(400/1000): loss=0.3823859143863311\n",
      "Gradient Descent(500/1000): loss=0.36432339685586884\n",
      "Gradient Descent(600/1000): loss=0.3554363683846993\n",
      "Gradient Descent(700/1000): loss=0.3504487458042329\n",
      "Gradient Descent(800/1000): loss=0.34735881390772444\n",
      "Gradient Descent(900/1000): loss=0.34531310270540333\n"
     ]
    }
   ],
   "source": [
    "loss, weights = gradient_descent(y,tX,1000,0.01)\n",
    "\n",
    "# it shoud be in this form\n",
    "# loss, weights = least_squares_GD(y, tx, initial w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Least Squares with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/999): loss=12.15898165685138\n",
      "SGD(100/999): loss=1.8835183660768777\n",
      "SGD(200/999): loss=0.9430653695280232\n",
      "SGD(300/999): loss=0.685508990984819\n",
      "SGD(400/999): loss=0.6171848929522746\n",
      "SGD(500/999): loss=0.5428422863277389\n",
      "SGD(600/999): loss=0.5305112361148754\n",
      "SGD(700/999): loss=0.46798035562992296\n",
      "SGD(800/999): loss=0.47489436532791773\n",
      "SGD(900/999): loss=0.44111256205958677\n"
     ]
    }
   ],
   "source": [
    "# yet to be implemented\n",
    "loss, weights = stochastic_gradient_descent(y, tX, initial_w=np.random.random(tX.shape[1]), batch_size=1, max_iters=1000, gamma=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Least Squares with Normal Equations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, weights = least_squares(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge regression with Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, weights = ridge_regression(y, tX, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression with Gradient Descent\n",
    "Riccardo : DOESN'T WORK, WHY?\n",
    "Raphael : It works but only with a super small step 0.000005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693147180559945 0\n",
      "0.5803475038000304 1\n",
      "0.5502308531740523 2\n",
      "0.5384949583794859 3\n",
      "0.5320769043136536 4\n",
      "0.5282002897636864 5\n",
      "0.5255101621333548 6\n",
      "0.5234598331501416 7\n",
      "0.521793164411905 8\n",
      "0.5203821807350647 9\n",
      "0.5191551127802628 10\n",
      "0.5180676844595974 11\n",
      "0.5170905560667799 12\n",
      "0.5162032283724981 13\n",
      "0.5153907968336295 14\n",
      "0.5146420631901972 15\n",
      "0.5139483783450218 16\n",
      "0.5133028997442223 17\n",
      "0.5127000996887792 18\n",
      "0.5121354301831335 19\n",
      "0.5116050890378668 20\n",
      "0.5111058530093209 21\n",
      "0.5106349564137393 22\n",
      "0.5101900012012914 23\n",
      "0.5097688892176785 24\n",
      "0.5093697703896856 25\n",
      "0.5089910025268377 26\n",
      "0.5086311197106667 27\n",
      "0.5082888070811556 28\n",
      "0.5079628803711713 29\n",
      "0.5076522688861173 30\n",
      "0.5073560008689407 31\n",
      "0.5070731904325735 32\n",
      "0.5068030255838172 33\n",
      "0.5065447573316542 34\n",
      "0.5062976903262155 35\n",
      "0.5060611756239194 36\n",
      "0.5058346058383197 37\n",
      "0.5056174123075718 38\n",
      "0.5054090634436958 39\n",
      "0.5052090634111787 40\n",
      "0.5050169506064579 41\n",
      "0.5048322957839487 42\n",
      "0.5046546999083992 43\n",
      "0.5044837918890258 44\n",
      "0.5043192263331966 45\n",
      "0.5041606814098023 46\n",
      "0.5040078568684473 47\n",
      "0.5038604722302751 48\n",
      "0.5037182651486576 49\n",
      "0.5035809899292175 50\n",
      "0.5034484161951147 51\n",
      "0.503320327682817 52\n",
      "0.5031965211541928 53\n",
      "0.5030768054119698 54\n",
      "0.5029610004069668 55\n",
      "0.5028489364268501 56\n",
      "0.5027404533574139 57\n",
      "0.5026354000084986 58\n",
      "0.5025336334976469 59\n",
      "0.5024350186854518 60\n",
      "0.5023394276573054 61\n",
      "0.5022467392468954 62\n",
      "0.5021568385973673 63\n",
      "0.5020696167565413 64\n",
      "0.5019849703030039 65\n",
      "0.5019028010002475 66\n",
      "0.5018230154763509 67\n",
      "0.5017455249269636 68\n",
      "0.5016702448395967 69\n",
      "0.5015970947374269 70\n",
      "0.5015259979410065 71\n",
      "0.5014568813464252 72\n",
      "0.501389675218617 73\n",
      "0.5013243129986237 74\n",
      "0.5012607311237345 75\n",
      "0.5011988688595317 76\n",
      "0.5011386681429374 77\n",
      "0.5010800734354567 78\n",
      "0.501023031585866 79\n",
      "0.5009674917016638 80\n",
      "0.5009134050286604 81\n",
      "0.5008607248381253 82\n",
      "0.5008094063209676 83\n",
      "0.5007594064884565 84\n",
      "0.500710684079032 85\n",
      "0.5006631994707911 86\n",
      "0.5006169145992615 87\n",
      "0.5005717928801082 88\n",
      "0.500527799136441 89\n",
      "0.500484899530418 90\n",
      "0.500443061498857 91\n",
      "0.500402253692595 92\n",
      "0.5003624459193464 93\n",
      "0.5003236090898326 94\n",
      "0.5002857151669705 95\n",
      "0.5002487371179195 96\n",
      "0.500212648868808 97\n",
      "0.5001774252619595 98\n",
      "0.5001430420154678 99\n",
      "0.5001094756849613 100\n",
      "0.5000767036274284 101\n",
      "0.500044703966966 102\n",
      "0.500013455562335 103\n",
      "0.49998293797620896 104\n",
      "0.4999531314460102 105\n",
      "0.4999240168562384 106\n",
      "0.499895575712197 107\n",
      "0.49986779011503596 108\n",
      "0.4998406427380305 109\n",
      "0.499814116804021 110\n",
      "0.4997881960639474 111\n",
      "0.4997628647764132 112\n",
      "0.49973810768822025 113\n",
      "0.49971391001581617 114\n",
      "0.49969025742760553 115\n",
      "0.4996671360270758 116\n",
      "0.4996445323366898 117\n",
      "0.49962243328250694 118\n",
      "0.4996008261794903 119\n",
      "0.4995796987174642 120\n",
      "0.4995590389476875 121\n",
      "0.4995388352700092 122\n",
      "0.49951907642057697 123\n",
      "0.4994997514600724 124\n",
      "0.4994808497624396 125\n",
      "0.4994623610040895 126\n",
      "0.4994442751535527 127\n",
      "0.4994265824615575 128\n",
      "0.49940927345151603 129\n",
      "0.4993923389103949 130\n",
      "0.49937576987995386 131\n",
      "0.4993595576483353 132\n",
      "0.4993436937419871 133\n",
      "0.49932816991790335 134\n",
      "0.49931297815616826 135\n",
      "0.49929811065278923 136\n",
      "0.4992835598128079 137\n",
      "0.49926931824367204 138\n",
      "0.499255378748861 139\n",
      "0.4992417343217508 140\n",
      "0.4992283781397103 141\n",
      "0.4992153035584161 142\n",
      "0.49920250410637695 143\n",
      "0.4991899734796607 144\n",
      "0.4991777055368132 145\n",
      "0.49916569429395957 146\n",
      "0.49915393392008456 147\n",
      "0.49914241873247917 148\n",
      "0.4991311431923497 149\n",
      "0.49912010190058287 150\n",
      "0.49910928959365686 151\n",
      "0.4990987011396986 152\n",
      "0.4990883315346719 153\n",
      "0.4990781758987021 154\n",
      "0.4990682294725236 155\n",
      "0.499058487614047 156\n",
      "0.49904894579504355 157\n",
      "0.49903959959794164 158\n",
      "0.49903044471272645 159\n",
      "0.4990214769339454 160\n",
      "0.4990126921578107 161\n",
      "0.4990040863793958 162\n",
      "0.4989956556899253 163\n",
      "0.49898739627414923 164\n",
      "0.49897930440780325 165\n",
      "0.49897137645514966 166\n",
      "0.4989636088665964 167\n",
      "0.49895599817639 168\n",
      "0.498948541000382 169\n",
      "0.49894123403386353 170\n",
      "0.4989340740494683 171\n",
      "0.49892705789513875 172\n",
      "0.49892018249215575 173\n",
      "0.49891344483322636 174\n",
      "0.4989068419806308 175\n",
      "0.4989003710644244 176\n",
      "0.498894029280694 177\n",
      "0.4988878138898655 178\n",
      "0.49888172221506083 179\n",
      "0.49887575164050507 180\n",
      "0.49886989960997846 181\n",
      "0.4988641636253134 182\n",
      "0.49885854124493617 183\n",
      "0.4988530300824499 184\n",
      "0.49884762780525765 185\n",
      "0.49884233213322554 186\n",
      "0.4988371408373825 187\n",
      "0.49883205173865913 188\n",
      "0.4988270627066587 189\n",
      "0.49882217165846465 190\n",
      "0.49881737655748076 191\n",
      "0.4988126754123036 192\n",
      "0.4988080662756248 193\n",
      "0.49880354724316517 194\n",
      "0.498799116452636 195\n",
      "0.498794772082731 196\n",
      "0.49879051235214256 197\n",
      "0.4987863355186066 198\n",
      "0.4987822398779729 199\n"
     ]
    }
   ],
   "source": [
    "loss, weights = logistic_regression_gradient_descent(y, tX)\n",
    "\n",
    "# it shoud be in this form\n",
    "# loss, weights = logistic regression_SGD(y, tX, initial w, max_iters, gamma)\n",
    "# and we should use SGd rather than GD because it's more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sigmoid(np.c_[np.ones((y.shape[0], 1)), tX]@weights)\n",
    "y_pred[y_pred <0.5] = -1\n",
    "y_pred[y_pred > 0.5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularized Logistic Regression with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yet to be implemented\n",
    "# loss, weights = reg_logistic regression_SGD(y, tX, initial w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "IDEA: insert CV in each of the methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, weights, best_lambda = cross_validation_demo(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate predictions and save ouput in csv format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/FIRSTsubmission.csv' \n",
    "y_pred = predict_labels(weights, np.c_[np.ones((y.shape[0], 1)), tX_test])\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION: we are prediction too LESS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs = np.count_nonzero(y_pred==1)\n",
    "print(f'From {y_pred.shape[0]} test examples, {higgs} are 1, i.e. the {higgs/y_pred.shape[0]} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
