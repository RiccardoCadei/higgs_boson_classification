{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"GHhNTfWx0AGu"},"source":["# Useful starting lines\n","import numpy as np\n","from helpers import *\n","from methods import *\n","from process_data import *\n","from crossValidation import *\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","seed=20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJdJ20A-0AG1"},"source":["# Load the dataset"]},{"cell_type":"code","metadata":{"id":"VNKVzVCh0B2g"},"source":["from zipfile import ZipFile \n","  \n","# # specifying the zip file name \n","file_name = 'Data/test.csv.zip'\n","  \n","# opening the zip file in READ mode \n","with ZipFile(file_name, 'r') as zip: \n","    zip.extractall('Data/') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIS5My0U0AG1"},"source":["y, tX, ids = load_csv_data('Data/train.csv')\n","_, tX_test, ids_test = load_csv_data('Data/test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cGxjbAte0AG6"},"source":["# Methods"]},{"cell_type":"markdown","metadata":{"id":"qhDrrEpc0AG7"},"source":["## 1. Least Squares with Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"c-C5gQC_0AG7"},"source":["#### Cross Validation"]},{"cell_type":"code","metadata":{"id":"6yMTilRb0AG8","outputId":"2e810048-012d-4944-da60-96ffdeee0a3e"},"source":["# Model parameters\n","max_iters = 3000\n","gamma = 0.005\n","\n","\n","# Split data in k-fold\n","k_fold = 2\n","k_indices = build_k_indices(y, k_fold, seed)\n","\n","\n","accs_train = []\n","accs_test = []\n","\n","for k in range(k_fold):\n","    acc_train, acc_test = cross_validation_least_squares_GD(y, tX, k_indices, k, max_iters, gamma)\n","    accs_train.append(acc_train)\n","    accs_test.append(acc_test)\n","    \n","for i in range(len(accs_train)):\n","    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n","\n","print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n","print(\"Variance test accuracy: %f\" % np.var(accs_test))\n","print(\"Min test accuracy: %f\" % np.min(accs_test))\n","print(\"Max test accuracy: %f\" % np.max(accs_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 - Training accuracy: 0.773952 / Test accuracy : 0.774912\n","1 - Training accuracy: 0.767112 / Test accuracy : 0.765072\n","\n","Average test accuracy: 0.769992\n","Variance test accuracy: 0.000024\n","Min test accuracy: 0.765072\n","Max test accuracy: 0.774912\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bkDIXO5f0AHD"},"source":["## 2. Least Squares with Stochastic Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"4zHFA3AM0AHE"},"source":["#### Cross Validation"]},{"cell_type":"code","metadata":{"id":"qMfrUMhn0AHF"},"source":["#TO DO\n","\n","initial_w=np.zeros(tX.shape[1])\n","batch_size=1\n","max_iters=1000\n","gamma=0.005\n","\n","loss, weights = least_squares_SGD(y, tX, initial_w, batch_size, max_iters, gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"trL4zoRQ0AHJ"},"source":["## 3. Least Squares with Normal Equations "]},{"cell_type":"markdown","metadata":{"id":"jcn-b3Ov0AHK"},"source":["#### Cross Validation"]},{"cell_type":"code","metadata":{"id":"ZBfwZaXh0AHL"},"source":["#TO DO \n","\n","loss, weights = least_squares(y, tX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRNkS9Vt0AHP"},"source":["## 4. Ridge regression with Normal Equations"]},{"cell_type":"markdown","metadata":{"id":"7FSx3D5s0AHQ"},"source":["#### Lambda"]},{"cell_type":"code","metadata":{"id":"CUHb8uzq0AHR"},"source":["# TO CHECK\n","\n","# To evaluate the best lambda that minimizes the test error\n","loss, weights, best_lambda = cross_validation_ridge_regression(y,tX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNtdvgWo0AHW"},"source":["#### Cross Validation"]},{"cell_type":"code","metadata":{"id":"2K8LmbWb0AHX","outputId":"33fa584e-49d2-48eb-89fd-6ac9da428a61"},"source":["# Model parameters\n","\n","lambdas = [0.01, 0.01, 0.01]\n","\n","# Split data in k-fold\n","k_fold = 2\n","k_indices = build_k_indices(y, k_fold, seed)\n","\n","\n","accs_train = []\n","accs_test = []\n","\n","for k in range(k_fold):\n","    acc_train, acc_test = cross_validation_ridge_regression(y, tX, k_indices, k, lambdas)\n","    accs_train.append(acc_train)\n","    accs_test.append(acc_test)\n","    \n","for i in range(len(accs_train)):\n","    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n","\n","print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n","print(\"Variance test accuracy: %f\" % np.var(accs_test))\n","print(\"Min test accuracy: %f\" % np.min(accs_test))\n","print(\"Max test accuracy: %f\" % np.max(accs_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 - Training accuracy: 0.782840 / Test accuracy : 0.783928\n","1 - Training accuracy: 0.782848 / Test accuracy : 0.781256\n","\n","Average test accuracy: 0.782592\n","Variance test accuracy: 0.000002\n","Min test accuracy: 0.781256\n","Max test accuracy: 0.783928\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D0qlFWFL0AHb"},"source":["## 5. Logistic Regression with Stochastic Gradient Descent\n"]},{"cell_type":"markdown","metadata":{"id":"0uYY_UbE0AHc"},"source":["#### Cross Validation"]},{"cell_type":"code","metadata":{"id":"tqdp4-eb0AHc"},"source":["# TO DO\n","\n","initial_w = np.random.random(tX.shape[1])\n","batch_size = 1\n","max_iters = 10000\n","gamma = 0.0009\n","\n","loss, weights = logistic_regression(y, tX, initial_w, batch_size, max_iters, gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGOLWOaT0AHg"},"source":["## 6. Regularized Logistic Regression with Stochastic Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"fyOb7sDh0AHh"},"source":["#### Optimal Lambda"]},{"cell_type":"code","metadata":{"id":"iy2RbgfS0AHh"},"source":["# TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WjGmRlLe0AHm"},"source":["#### Cross Validation"]},{"cell_type":"code","metadata":{"id":"Nj7QHy7E0AHn"},"source":["# TO DO\n","\n","lambda_ = 0.001\n","initial_w = np.random.random(tX.shape[1])\n","batch_size = 1\n","max_iters = 1000\n","gamma = 0.1\n","\n","loss, weights = reg_logistic_regression(y, tX, lambda_, initial_w, batch_size,  max_iters, gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wPuY4BlP0AHr"},"source":["# Prediction (file.run)\n","by now the best accuracy predicted is through RIDGE REGRESSION"]},{"cell_type":"code","metadata":{"id":"8zP-dXnh0AHr"},"source":["# Split data in subsets corresponding to a jet value\n","msks_jet_train = get_jet_masks(tX)\n","msks_jet_test = get_jet_masks(tX_test)\n","\n","# Ridge regression parameters for each subset\n","lambdas = [0.01, 0.01, 0.01]\n","\n","# Vector to store the final prediction\n","y_pred = np.zeros(tX_test.shape[0])\n","\n","for idx in range(len(msks_jet_train)):\n","    x_train = tX[msks_jet_train[idx]]\n","    x_test = tX_test[msks_jet_test[idx]]\n","    y_train = y[msks_jet_train[idx]]\n","\n","    # Pre-processing of data\n","    x_train, x_test = process_data(x_train, x_test, True)\n","\n","    loss, weights = ridge_regression(y_train, x_train, lambdas[idx])\n","\n","    y_test_pred = predict_labels(weights, x_test)\n","\n","    y_pred[msks_jet_test[idx]] = y_test_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caDm4rbc0AHx","outputId":"3d109399-41cf-4a15-b840-a10ef1eecf00"},"source":["higgs = np.count_nonzero(y_pred==1)\n","print(f'From {y_pred.shape[0]} test examples, {higgs} are 1, i.e. the {higgs/y_pred.shape[0]} %')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["From 568238 test examples, 161790 are 1, i.e. the 0.28472224666424983 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mfjj7aBa0AH5"},"source":["#### Generate predictions and save ouput in csv format for submission"]},{"cell_type":"code","metadata":{"id":"Ub5ITgmg0AH5"},"source":["OUTPUT_PATH = 'data/RidgeRegression.csv' \n","create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuQONAGy0AH9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_3W_bxL0AIA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fs6Nkx-q0AIE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KoefpFkD0AIH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJimq_Yz0AIL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfJHyeRX0AIO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCbdi7wq0AIR"},"source":["# OTHERS (old)"]},{"cell_type":"markdown","metadata":{"id":"PXTtZSbs0AIS"},"source":["### Umbalanced Dataset"]},{"cell_type":"code","metadata":{"id":"2GimxSef0AIS"},"source":["higgs = np.count_nonzero(y==1)\n","print(f'From {y.shape[0]} training examples, {higgs} are 1, i.e. the {higgs/y.shape[0]} %')\n","\n","# Random Over Sampling\n","#tX, y = Random_Over_Sampling(tX, y)\n","\n","#higgs = np.count_nonzero(y==1)\n","#print(f'Applying Random Over Sampling: \\nFrom {y.shape[0]} training examples, {higgs} are 1, i.e. the {higgs/y.shape[0]} %')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3hPd3D80AIX"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"1KoJPkg50AIY"},"source":["tX, tX_test = process_data(tX, tX_test, add_constant_col=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yx3zjeX60AIe"},"source":["# Cross Validation\n","IDEA: insert CV in each of the methods above"]},{"cell_type":"code","metadata":{"id":"QFR2Y75w0AIf"},"source":["def cross_validation(y, x, k_indices, k, regression_method, **args):\n","    \"\"\"\n","    Completes k-fold cross-validation using the regression method\n","    passed as argument.\n","    \"\"\"\n","    # get k'th subgroup in test, others in train\n","    msk_test = k_indices[k]\n","    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n","\n","    x_train = x[msk_train, :]\n","    x_test = x[msk_test, :]\n","    y_train = y[msk_train]\n","    y_test = y[msk_test]\n","\n","    # data pre-processing\n","    #x_train, x_test = process_data(x_train, x_test, True)\n","\n","    # compute weights using given method\n","    loss, weights = regression_method(y=y_train, tx=x_train, **args)\n","    \n","    # predict output for train and test data\n","    y_train_pred = predict_labels(weights, x_train)\n","    y_test_pred = predict_labels(weights, x_test)\n","    \n","    \n","    # compute accuracy for train and test data\n","    acc_train = compute_accuracy(y_train_pred, y_train)\n","    acc_test = compute_accuracy(y_test_pred, y_test)\n","\n","    return acc_train, acc_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSzxeArR0AIj","outputId":"7efa3077-566c-45a4-d95b-d51c538dd74c"},"source":["regression_method = ridge_regression\n","\n","# Model parameters\n","lambda_ = 0.0005\n","\n","# Split data in k-fold\n","k_fold = 2\n","k_indices = build_k_indices(y, k_fold, seed)\n","\n","\n","accs_train = []\n","accs_test = []\n","\n","for k in range(k_fold):\n","    acc_train, acc_test = cross_validation(y, tX, k_indices, k, regression_method, lambda_=lambda_)\n","    accs_train.append(acc_train)\n","    accs_test.append(acc_test)\n","    \n","for i in range(len(accs_train)):\n","    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n","\n","print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n","print(\"Variance test accuracy: %f\" % np.var(accs_test))\n","print(\"Min test accuracy: %f\" % np.min(accs_test))\n","print(\"Max test accuracy: %f\" % np.max(accs_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 - Training accuracy: 0.775480 / Test accuracy : 0.776096\n","1 - Training accuracy: 0.775888 / Test accuracy : 0.774656\n","\n","Average test accuracy: 0.775376\n","Variance test accuracy: 0.000001\n","Min test accuracy: 0.774656\n","Max test accuracy: 0.776096\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"97-LOyA10AIp"},"source":["# Only for non logistic methods\n","y_pred = predict_labels(weights, tX_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8VXKujj0AIw"},"source":["# Only for Logistic methods\n","y_pred = sigmoid(tX_test@weights)\n","y_pred[y_pred <0.5] = -1\n","y_pred[y_pred > 0.5] = 1"],"execution_count":null,"outputs":[]}]}