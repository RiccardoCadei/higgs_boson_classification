{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "from methods import *\n",
    "from process_data import *\n",
    "from crossValidation import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data('Data/train.csv')\n",
    "_, tX_test, ids_test = load_csv_data('Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 250000 training examples, 85667 are 1, i.e. the 0.342668 %\n",
      "Applying Random Over Sampling: \n",
      "From 328666 training examples, 164333 are 1, i.e. the 0.5 %\n"
     ]
    }
   ],
   "source": [
    "higgs = np.count_nonzero(y==1)\n",
    "print(f'From {y.shape[0]} training examples, {higgs} are 1, i.e. the {higgs/y.shape[0]} %')\n",
    "\n",
    "# Random Over Sampling\n",
    "#tX, y = Random_Over_Sampling(tX, y)\n",
    "\n",
    "#higgs = np.count_nonzero(y==1)\n",
    "#print(f'Applying Random Over Sampling: \\nFrom {y.shape[0]} training examples, {higgs} are 1, i.e. the {higgs/y.shape[0]} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPERIDEA: SLIT THE DATASET IN 4 CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ideas\n",
    "\n",
    "1. New features: Apply a polynomial basis to all the X features\n",
    "\n",
    "2. PCA, correlation analysis (scatterplot, VIF, ...), manage the 0s in the last feature\n",
    "\n",
    "3. Outlayer analysis, leverages, cook's metric ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DON'T NEED THIS FOR RIDGE\n",
    "\n",
    "tX, tX_test = process_data(tX, tX_test, add_constant_col=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Least Squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=200\n",
    "gamma=0.005\n",
    "\n",
    "loss, weights = least_squares_GD(y, tX,initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Least Squares with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w=np.zeros(tX.shape[1])\n",
    "batch_size=1\n",
    "max_iters=1000\n",
    "gamma=0.005\n",
    "\n",
    "loss, weights = least_squares_SGD(y, tX, initial_w, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Least Squares with Normal Equations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, weights = least_squares(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge regression with Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CHECK\n",
    "\n",
    "# To evaluate the best lambda that minimizes the test error\n",
    "loss, weights, best_lambda = cross_validation_ridge_regression(y,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.782840 / Test accuracy : 0.783928\n",
      "1 - Training accuracy: 0.782848 / Test accuracy : 0.781256\n",
      "\n",
      "Average test accuracy: 0.782592\n",
      "Variance test accuracy: 0.000002\n",
      "Min test accuracy: 0.781256\n",
      "Max test accuracy: 0.783928\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "\n",
    "lambdas = [0.01, 0.01, 0.01]\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 2\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_ridge_regression(y, tX, k_indices, k, lambdas)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT SPLITTING THE DATASET\n",
    "\n",
    "lambda_ = 0.01\n",
    "\n",
    "loss, weights = ridge_regression(y, tX, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in subsets corresponding to a jet value\n",
    "msks_jet_train = get_jet_masks(tX)\n",
    "msks_jet_test = get_jet_masks(tX_test)\n",
    "\n",
    "# Ridge regression parameters for each subset\n",
    "lambdas = [0.01, 0.01, 0.01]\n",
    "\n",
    "# Vector to store the final prediction\n",
    "y_pred = np.zeros(tX_test.shape[0])\n",
    "\n",
    "for idx in range(len(msks_jet_train)):\n",
    "    x_train = tX[msks_jet_train[idx]]\n",
    "    x_test = tX_test[msks_jet_test[idx]]\n",
    "    y_train = y[msks_jet_train[idx]]\n",
    "\n",
    "    # Pre-processing of data\n",
    "    x_train, x_test = process_data(x_train, x_test, True)\n",
    "\n",
    "    loss, weights = ridge_regression(y_train, x_train, lambdas[idx])\n",
    "\n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "\n",
    "    y_pred[msks_jet_test[idx]] = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression with Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.random.random(tX.shape[1])\n",
    "batch_size = 1\n",
    "max_iters = 10000\n",
    "gamma = 0.0009\n",
    "\n",
    "loss, weights = logistic_regression(y, tX, initial_w, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularized Logistic Regression with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.001\n",
    "initial_w = np.random.random(tX.shape[1])\n",
    "batch_size = 1\n",
    "max_iters = 1000\n",
    "gamma = 0.1\n",
    "\n",
    "loss, weights = reg_logistic_regression(y, tX, lambda_, initial_w, batch_size,  max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "IDEA: insert CV in each of the methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.775480 / Test accuracy : 0.776096\n",
      "1 - Training accuracy: 0.775888 / Test accuracy : 0.774656\n",
      "\n",
      "Average test accuracy: 0.775376\n",
      "Variance test accuracy: 0.000001\n",
      "Min test accuracy: 0.774656\n",
      "Max test accuracy: 0.776096\n"
     ]
    }
   ],
   "source": [
    "regression_method = ridge_regression\n",
    "\n",
    "# Model parameters\n",
    "lambda_ = 0.0005\n",
    "\n",
    "# Split data in k-fold\n",
    "k_fold = 2\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation(y, tX, k_indices, k, regression_method, lambda_=lambda_)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate predictions and save ouput in csv format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for non logistic methods\n",
    "y_pred = predict_labels(weights, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Logistic methods\n",
    "y_pred = sigmoid(tX_test@weights)\n",
    "y_pred[y_pred <0.5] = -1\n",
    "y_pred[y_pred > 0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/FIRSTsubmission.csv' \n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION: we are prediction too LESS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 568238 test examples, 161790 are 1, i.e. the 0.28472224666424983 %\n"
     ]
    }
   ],
   "source": [
    "higgs = np.count_nonzero(y_pred==1)\n",
    "print(f'From {y_pred.shape[0]} test examples, {higgs} are 1, i.e. the {higgs/y_pred.shape[0]} %')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
